{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cd3dbf-87f5-4f51-ae6f-ac332f5a74cb",
   "metadata": {},
   "source": [
    "# Simulated transactions analysis - Main Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31770849-7892-4d25-b25c-4f8d61f29ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b788e6b-aca0-49d3-b171-94358d8e4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3ba76b-62dc-4045-87b7-51c83c00f757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDate\r\n",
       "import java.time.format.DateTimeFormatter\r\n",
       "import java.time.temporal.ChronoUnit\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.temporal.ChronoUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c9f04-0a9e-4482-bfe5-3539354c57c0",
   "metadata": {},
   "source": [
    "## Case classes \n",
    "\n",
    "After making imports, Transaction and CustomerSpending case classes are instanciated, in order to maintain a more readable code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bcfb2df-ed0d-4e7c-aafe-ed0c7ee6cdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Transaction\r\n",
       "defined class CustomerSpending\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// case class definition for data clarity\n",
    "\n",
    "case class Transaction(\n",
    "  custId: String,\n",
    "  startDate: String,\n",
    "  endDate: String,\n",
    "  transId: String,\n",
    "  date: LocalDate,\n",
    "  year: Int,\n",
    "  month: Int,\n",
    "  day: Int,\n",
    "  expType: String,\n",
    "  amount: Double\n",
    ")\n",
    "\n",
    "case class CustomerSpending(totalSpend: Double, spendingClass: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec320989-8db7-4822-a091-18c7414e2797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = sample_data_big_data.csv MapPartitionsRDD[1] at textFile at <console>:34\r\n",
       "header: String = CUST_ID,START_DATE,END_DATE,TRANS_ID,DATE,YEAR,MONTH,DAY,EXP_TYPE,AMOUNT\r\n",
       "dataRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:36\r\n",
       "transactionsRdd: org.apache.spark.rdd.RDD[Transaction] = MapPartitionsRDD[3] at map at <console>:39\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// rdd mapping with created custom case classes \n",
    "\n",
    "val rdd = sc.textFile(\"sample_data_big_data.csv\")\n",
    "val header = rdd.first()\n",
    "val dataRdd = rdd.filter(row => row != header)\n",
    "\n",
    "\n",
    "val transactionsRdd = dataRdd.map { line =>\n",
    "    val fields = line.split(\",\")\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
    "    Transaction(\n",
    "        custId   = fields(0),\n",
    "        startDate = fields(1),\n",
    "        endDate   = fields(2),\n",
    "        transId   = fields(3),\n",
    "        date      = LocalDate.parse(fields(4), formatter),\n",
    "        year      = fields(5).toInt,\n",
    "        month     = fields(6).toInt,\n",
    "        day       = fields(7).toInt,\n",
    "        expType   = fields(8),\n",
    "        amount    = fields(9).toDouble\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee095fc8-d026-4734-a0b6-0c40cf49f10f",
   "metadata": {},
   "source": [
    "## Classified spendings\n",
    "\n",
    "First thing first, it's needed to understand every single customer spends, in order to obtain the spending class.\n",
    "After that, a join is made between transactions and classified spendings, in order to obtain a more complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0da1149-5b89-456c-991e-14e754497c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerSpendingsRdd: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[5] at reduceByKey at <console>:35\r\n",
       "classifiedSpendingsRdd: org.apache.spark.rdd.RDD[(String, CustomerSpending)] = MapPartitionsRDD[6] at map at <console>:38\r\n",
       "transactionsJoinedRdd: org.apache.spark.rdd.RDD[(String, String, String, Int, String, Double)] = MapPartitionsRDD[11] at map at <console>:49\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// total spendings per customer\n",
    "val customerSpendingsRdd = transactionsRdd\n",
    "    .map(t => (t.custId, t.amount))\n",
    "    .reduceByKey(_ + _)\n",
    "\n",
    "// customer spendings classification\n",
    "val classifiedSpendingsRdd = customerSpendingsRdd.map { case (custId, totalSpend) =>\n",
    "    val spendingClass = if (totalSpend < 1000.0) \"Poor\"\n",
    "                      else if (totalSpend < 10000.0) \"Middle\"\n",
    "                      else \"Rich\"\n",
    "    (custId, CustomerSpending(totalSpend, spendingClass))\n",
    "}\n",
    "\n",
    "// inner join transactions with classified spendings\n",
    "val transactionsJoinedRdd = transactionsRdd\n",
    "    .map(t => (t.custId, t))\n",
    "    .join(classifiedSpendingsRdd)  // (custId, (Transaction, CustomerSpending))\n",
    "    .map { case (custId, (transaction, custSpending)) =>\n",
    "        (transaction.custId, transaction.transId, custSpending.spendingClass, transaction.year, transaction.expType, transaction.amount)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f39e7d-4252-437a-a601-a97d2d15a927",
   "metadata": {},
   "source": [
    "### Spending Analysis by Category and Year\n",
    "\n",
    "This job aims to aggregate customer transactions based on **spending class** (`spendingClass`) and **year** (`year`), computing key metrics for each combination.\n",
    "\n",
    "#### **Main Steps**:\n",
    "1. **Transaction Aggregation**  \n",
    "   - Transactions are grouped by (`spendingClass`, `year`).  \n",
    "   - The total spending amount (`totalAmount`) is calculated.  \n",
    "   - The unique set of customers who made purchases (`custSet`) is collected.  \n",
    "\n",
    "2. **Metric Calculation**  \n",
    "   - The total number of customers (`customerCount`) for each (`spendingClass`, `year`).  \n",
    "   - The average spending per customer (`avgSpend = totalAmount / customerCount`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a9f9d69-a2e7-4d80-b7f3-9d80363c9b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregatedByCategoryYear: org.apache.spark.rdd.RDD[((String, Int), (Double, scala.collection.immutable.Set[String]))] = ShuffledRDD[13] at reduceByKey at <console>:37\r\n",
       "metricsByCategoryYear: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double))] = MapPartitionsRDD[14] at map at <console>:41\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// criteria: (spending class, year)\n",
    "\n",
    "// Mapping: ((spendingClass, year), (amount, Set(customerId)))\n",
    "val aggregatedByCategoryYear = transactionsJoinedRdd\n",
    "    .map { case (custId, transId, spendingClass, year, expType, amount) =>\n",
    "        ((spendingClass, year), (amount, Set(custId)))\n",
    "    }\n",
    "    .reduceByKey { case ((amount1, custSet1), (amount2, custSet2)) =>\n",
    "        (amount1 + amount2, custSet1 ++ custSet2)\n",
    "    }\n",
    "\n",
    "val metricsByCategoryYear = aggregatedByCategoryYear.map { case ((spendingClass, year), (totalAmount, custSet)) =>\n",
    "  val customerCount = custSet.size\n",
    "  val avgSpend = totalAmount / customerCount\n",
    "  ((spendingClass, year), (totalAmount, customerCount, avgSpend))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec04a74-80de-4827-805e-8a9090529b61",
   "metadata": {},
   "source": [
    "## Spending trends\n",
    "\n",
    "Previous calculated metrics can be useful, and I choose to use them in order to calculate spending trends, based on growth rate and retention rate.\n",
    "\n",
    "#### **Main Steps**:  \n",
    "1. **Grouping Data by Spending Class**:  \n",
    "   - dataset mapped to `(spendingClass, (year, totalAmount, customerCount, avgSpend))` to facilitate trend analysis.  \n",
    "   - Then, we use `groupByKey()` to aggregate all years for each `spendingClass`.\n",
    "2. **Sorting by Year**:  \n",
    "   - Inside each spending class group, data is sorted by `year` to track changes over time.  \n",
    "3. **Calculating Growth Rate and Retention Rate**:  \n",
    "   - consecutive years are confronted in order to get significant indexes.  \n",
    "   - **Growth Rate** is computed as the percentage change in `avgSpend` from the previous year.  \n",
    "   - **Retention Rate** is calculated as the percentage of customers retained from the previous year.  \n",
    "   - If there is no previous year, the values default to `0.0%` growth and `100%` retention.  \n",
    "4. **Flattening the Output**:  \n",
    "   - structured dataset `(spendingClass, year, totalAmount, customerCount, avgSpend, growthRate, retentionRate)`, which can be further analyzed or visualized.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12a5689-f7b8-4a41-b5f9-130ef40dc238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spendingTrends: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double, Double, Double))] = MapPartitionsRDD[17] at flatMap at <console>:35\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spendingTrends = metricsByCategoryYear\n",
    "    .map { case ((spendingClass, year), (totalAmount, customerCount, avgSpend)) =>\n",
    "        (spendingClass, (year, totalAmount, customerCount, avgSpend))\n",
    "    }\n",
    "    .groupByKey() \n",
    "    .flatMap { case (spendingClass, data) =>\n",
    "        val sortedData = data.toList.sortBy(_._1)\n",
    "    \n",
    "        var prevAvgSpend = 0.0\n",
    "        var prevCustCount = 0\n",
    "        var result = List.empty[((String, Int), (Double, Int, Double, Double, Double))]\n",
    "        \n",
    "        sortedData.map { case (year, totalAmount, customerCount, avgSpend) =>\n",
    "          val growthRate = if (prevAvgSpend > 0) ((avgSpend - prevAvgSpend) / prevAvgSpend) * 100 else 0.0\n",
    "          val retentionRate = if (prevCustCount > 0) (customerCount.toDouble / prevCustCount) * 100 else 100.0\n",
    "          \n",
    "          prevAvgSpend = avgSpend\n",
    "          prevCustCount = customerCount\n",
    "    \n",
    "          result = result :+ ((spendingClass, year), (totalAmount, customerCount, avgSpend, growthRate, retentionRate))\n",
    "        }\n",
    "    \n",
    "        result\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f54b8456-65e1-463a-90ec-3d99e432a038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((Middle,2010),(3079.2499999999995,10,307.92499999999995,0.0,100.0))\n",
      "((Middle,2011),(32507.629999999997,39,833.5289743589743,170.69220568611655,390.0))\n",
      "((Middle,2012),(56130.0,62,905.3225806451613,8.613210637506622,158.97435897435898))\n",
      "((Middle,2013),(66541.70999999999,81,821.5025925925925,-9.258576980686389,130.64516129032256))\n",
      "((Middle,2014),(84076.24999999999,105,800.7261904761904,-2.529073225543152,129.62962962962962))\n",
      "((Middle,2015),(102454.96000000002,116,883.2324137931037,10.303924649679185,110.47619047619048))\n",
      "((Middle,2016),(114106.84000000001,138,826.8611594202899,-6.38238061607402,118.96551724137932))\n",
      "((Middle,2017),(146244.90000000002,161,908.3534161490685,9.855615516625857,116.66666666666667))\n",
      "((Middle,2018),(161478.65,156,1035.1195512820511,13.955596233721792,96.8944099378882))\n",
      "((Middle,2019),(182600.3,161,1134.163354037267,9.56834431661008,103.20512820512822))\n",
      "((Middle,2020),(221567.11999999997,165,1342.83103030303,18.39837934482465,102.48447204968944))\n",
      "((Poor,2010),(29576.680000000004,639,46.28588419405322,0.0,100.0))\n",
      "((Poor,2011),(83349.50000000004,1709,48.77091866588651,5.368881928267389,267.4491392801252))\n",
      "((Poor,2012),(134143.23999999996,2652,50.581915535444935,3.713272005321379,155.17846693973084))\n",
      "((Poor,2013),(201802.95000000007,3538,57.03870830977956,12.765022253477271,133.40874811463047))\n",
      "((Poor,2014),(234921.54000000027,4055,57.9337953144267,1.5692624029735929,114.6127755794234))\n",
      "((Poor,2015),(272362.26999999984,4639,58.71141840913987,1.34226161171167,114.4019728729963))\n",
      "((Poor,2016),(342843.3500000002,5263,65.14219076572302,10.953188546339124,113.45117482215994))\n",
      "((Poor,2017),(370305.6299999997,5553,66.6856888168557,2.369429141067291,105.51016530495916))\n",
      "((Poor,2018),(412025.0099999999,5791,71.1491987566914,6.693355079669644,104.28597154691157))\n",
      "((Poor,2019),(445466.97999999963,5858,76.04420962785927,6.879924098523324,101.1569677085132))\n",
      "((Poor,2020),(436185.15000000043,5322,81.95887824126277,7.7779342337153,90.8501194947081))\n"
     ]
    }
   ],
   "source": [
    "spendingTrends.collect().toSeq\n",
    "    .sortBy { case ((spendingClass, year), _) => (spendingClass, year) }\n",
    "    .foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdcaea-4067-46db-b697-72a3aab4eb39",
   "metadata": {},
   "source": [
    "# Job optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c625f-42f6-4e4e-a07c-a2f617bc9d7e",
   "metadata": {},
   "source": [
    "## caching transactionsRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58e00cbe-1b40-4448-8aa5-d2258af5de74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = sample_data_big_data.csv MapPartitionsRDD[19] at textFile at <console>:44\r\n",
       "header: String = CUST_ID,START_DATE,END_DATE,TRANS_ID,DATE,YEAR,MONTH,DAY,EXP_TYPE,AMOUNT\r\n",
       "dataRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at filter at <console>:46\r\n",
       "transactionsRdd: org.apache.spark.rdd.RDD[Transaction] = MapPartitionsRDD[21] at map at <console>:48\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// rdd mapping with created custom case classes \n",
    "\n",
    "val rdd = sc.textFile(\"sample_data_big_data.csv\")\n",
    "val header = rdd.first()\n",
    "val dataRdd = rdd.filter(row => row != header)\n",
    "\n",
    "val transactionsRdd = dataRdd.map { line =>\n",
    "    val fields = line.split(\",\")\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
    "    Transaction(\n",
    "        custId   = fields(0),\n",
    "        startDate = fields(1),\n",
    "        endDate   = fields(2),\n",
    "        transId   = fields(3),\n",
    "        date      = LocalDate.parse(fields(4), formatter),\n",
    "        year      = fields(5).toInt,\n",
    "        month     = fields(6).toInt,\n",
    "        day       = fields(7).toInt,\n",
    "        expType   = fields(8),\n",
    "        amount    = fields(9).toDouble\n",
    "    )\n",
    "}.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a92482-95cc-460f-8524-b77b9901dff5",
   "metadata": {},
   "source": [
    "## Using broadcast for segmentation\n",
    "\n",
    "Since it's just a simple treshold, broadcast can drastically improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6baaa4f5-6bbd-438c-8f7f-1fcbde2b9d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerSpendingsRdd: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[23] at reduceByKey at <console>:42\r\n",
       "spendingThresholds: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Double]] = Broadcast(4)\r\n",
       "classifiedSpendingsRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[24] at map at <console>:49\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// total spendings per customer\n",
    "val customerSpendingsRdd = transactionsRdd\n",
    "    .map(t => (t.custId, t.amount))\n",
    "    .reduceByKey(_ + _)\n",
    "\n",
    "val spendingThresholds = sc.broadcast(Map(\n",
    "    \"Poor\" -> 1000.0,\n",
    "    \"Middle\" -> 10000.0\n",
    "))\n",
    "\n",
    "val classifiedSpendingsRdd = customerSpendingsRdd.map { case (custId, totalSpend) =>\n",
    "  val spendingClass = if (totalSpend < spendingThresholds.value(\"Poor\")) \"Poor\"\n",
    "                      else if (totalSpend < spendingThresholds.value(\"Middle\")) \"Middle\"\n",
    "                      else \"Rich\"\n",
    "  (custId, spendingClass)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641f375-f4c1-4968-9a14-ed01564c4794",
   "metadata": {},
   "source": [
    "# Using broadcast for joining \n",
    "\n",
    "In this case, I considered broadcasting classifiedSpendingsRddd, because it has 75000 records (unique customer id's), while transactionsRdd has more than 200 millions. Other than that, a more selective field projection has been made, to reduce computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd7dd5ad-dfb7-41dd-afcd-55a8336a06a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifiedSpendingsMap: scala.collection.Map[String,String] = Map(C6HWPBA456 -> Poor, CSAME0IGFA -> Poor, CVKB0FW9XD -> Poor, CB6ZIL1FOE -> Poor, CAKHPDBEYK -> Poor, CEV9KHT8PH -> Poor, CEXLHX94GW -> Poor, C7RERY5SLC -> Poor, C7ZKY7VORL -> Poor, C2SYBFN10E -> Poor, C8H2G6WL39 -> Poor, CW9OO8ACPX -> Poor, CIYCXH14HL -> Poor, CU68AVKSGS -> Poor, CH6WHATYFG -> Poor, CWXB07DHWH -> Poor, C9OK7OCTOO -> Poor, CPCDVVV6GT -> Poor, CIYOINA6LX -> Poor, CLMO3I9RBB -> Poor, CZR1FMJIO6 -> Poor, CS54URQ7ZE -> Poor, CLR1OUDAH0 -> Poor, CUIXFN8TD9 -> Poor, C1WQZF7SMV -> Poor, CZ5IXCDEOA -> Poor, C5YT7KLA4R -> Poor, C8AO6TI076 -> Poor, C7V213LH7A -> Poor, CO2JDKV6TY -> Poor, C9QT9NZM7V -> Poor, CNR371OZGW -> Poor, CDBJSATHXU -> Poor, CTJ9E9HHUC -> Poor, CHQGM5Q96H -> Poor, C96J0FWPQ7 -> Poor, C508JHG3EL ...\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifiedSpendingsMap = classifiedSpendingsRdd.collectAsMap()\n",
    "val broadcastspendings = sc.broadcast(classifiedSpendingsMap)\n",
    "\n",
    "val transactionsJoinedRdd = transactionsRdd.map { t =>\n",
    "    val spendingClass = broadcastspendings.value.getOrElse(t.custId, \"Unknown\")\n",
    "    ((spendingClass, t.year), (t.amount, 1))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395984e-94c6-46a4-a783-aa49701fac8e",
   "metadata": {},
   "source": [
    "## Removing useless fields (no broadcast alternative)\n",
    "\n",
    "This optimization is pretty simple but effective, this rdd only has 4 fields, since other ones are not used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "922f94fb-76a8-4aa7-ae10-ecdfb0edd4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transactionsJoinedRdd: org.apache.spark.rdd.RDD[((String, Int), (Double, Int))] = MapPartitionsRDD[30] at map at <console>:41\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transactionsJoinedRdd = transactionsRdd\n",
    "    .map(t => (t.custId, (t.transId, t.year, t.expType, t.amount)))\n",
    "    .join(classifiedSpendingsRdd)\n",
    "    .map { case (custId, ((transId, year, expType, amount), spendingClass)) =>\n",
    "        ((spendingClass, year), (amount, 1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bae98d1-d94b-4a2a-8f00-1e3151bd9a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metricsByCategoryYear: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double))] = MapPartitionsRDD[32] at map at <console>:41\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metricsByCategoryYear = transactionsJoinedRdd\n",
    "  .reduceByKey { case ((amount1, count1), (amount2, count2)) =>\n",
    "    (amount1 + amount2, count1 + count2)\n",
    "  }\n",
    "  .map { case ((spendingClass, year), (totalAmount, customerCount)) =>\n",
    "    val avgSpend = totalAmount / customerCount\n",
    "    ((spendingClass, year), (totalAmount, customerCount, avgSpend))\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868cbb1-a71f-4643-bba6-b09b36b59056",
   "metadata": {},
   "source": [
    "## GroupByKey (optimization 1)\n",
    "\n",
    "As it was stated in classes, even though GroupByKey is simple to understand, it can be bad performance wise, that's why in the optimization, aggregate by key was preferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85272967-2532-42b1-b2d8-9869c9011c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spendingTrends: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double, Double, Double))] = MapPartitionsRDD[35] at flatMap at <console>:46\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spendingTrends = metricsByCategoryYear\n",
    "    .map { case ((spendingClass, year), (totalAmount, customerCount, avgSpend)) =>\n",
    "        (spendingClass, (year, totalAmount, customerCount, avgSpend))\n",
    "    }\n",
    "    .aggregateByKey(List.empty[(Int, Double, Int, Double)])(\n",
    "        (acc, value) => acc :+ value,  \n",
    "        \n",
    "        (acc1, acc2) => (acc1 ++ acc2).sortBy(_._1)  \n",
    "    )\n",
    "    .flatMap { case (spendingClass, sortedData) =>\n",
    "        var prevAvgSpend = 0.0\n",
    "        var prevCustCount = 0\n",
    "        \n",
    "        sortedData.map { case (year, totalAmount, customerCount, avgSpend) =>\n",
    "          val growthRate = if (prevAvgSpend > 0) ((avgSpend - prevAvgSpend) / prevAvgSpend) * 100 else 0.0\n",
    "          val retentionRate = if (prevCustCount > 0) (customerCount.toDouble / prevCustCount) * 100 else 100.0\n",
    "        \n",
    "          prevAvgSpend = avgSpend\n",
    "          prevCustCount = customerCount\n",
    "    \n",
    "        ((spendingClass, year), (totalAmount, customerCount, avgSpend, growthRate, retentionRate))\n",
    "    }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c124d0-5c37-413e-b11c-f07112ddec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((Middle,2010),(3079.2499999999995,11,279.93181818181813,0.0,100.0))\n",
      "((Middle,2011),(32507.629999999997,42,773.9911904761905,176.49275295081907,381.8181818181818))\n",
      "((Middle,2012),(56130.0,69,813.4782608695652,5.1017467484456445,164.28571428571428))\n",
      "((Middle,2013),(66541.70999999999,93,715.502258064516,-12.044083722694454,134.7826086956522))\n",
      "((Middle,2014),(84076.24999999999,121,694.8450413223139,-2.887093158599011,130.10752688172042))\n",
      "((Middle,2015),(102454.96000000002,133,770.3380451127821,10.864725125878769,109.91735537190081))\n",
      "((Middle,2016),(114106.84000000001,162,704.3632098765432,-8.56440048038647,121.80451127819549))\n",
      "((Middle,2017),(146244.90000000002,190,769.7100000000002,9.27742806653836,117.28395061728396))\n",
      "((Middle,2018),(161478.65,192,841.0346354166667,9.266429618514312,101.05263157894737))\n",
      "((Middle,2019),(182600.3,196,931.6341836734694,10.77239205635303,102.08333333333333))\n",
      "((Middle,2020),(221567.11999999997,186,1191.221075268817,27.863607427089736,94.89795918367348))\n",
      "((Poor,2010),(29576.680000000004,679,43.559175257731965,0.0,100.0))\n",
      "((Poor,2011),(83349.50000000004,1839,45.323273518216446,4.049889030374478,270.839469808542))\n",
      "((Poor,2012),(134143.23999999996,2842,47.20029556650245,4.1414088228459125,154.54051114736268))\n",
      "((Poor,2013),(201802.95000000007,3837,52.59394057857703,11.42714245184176,135.01055594651655))\n",
      "((Poor,2014),(234921.54000000027,4388,53.537269826800426,1.7936082328990508,114.36017722178786))\n",
      "((Poor,2015),(272362.26999999984,5018,54.27705659625346,1.3818163904254614,114.35733819507747))\n",
      "((Poor,2016),(342843.3500000002,5704,60.10577664796637,10.73882855341726,113.67078517337586))\n",
      "((Poor,2017),(370305.6299999997,5990,61.82063939899828,2.8530747736206643,105.01402524544179))\n",
      "((Poor,2018),(412025.0099999999,6238,66.05081917281179,6.842665839334634,104.14023372287144))\n",
      "((Poor,2019),(445466.97999999963,6311,70.58579939787667,6.865895505701161,101.17024687399807))\n",
      "((Poor,2020),(436185.15000000043,5759,75.73973780170176,7.301664708468447,91.25336713674537))\n"
     ]
    }
   ],
   "source": [
    "spendingTrends.collect().toSeq\n",
    "    .sortBy { case ((spendingClass, year), _) => (spendingClass, year) }\n",
    "    .foreach(println)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dca46c9-8195-4030-af63-4c626a3b3b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
